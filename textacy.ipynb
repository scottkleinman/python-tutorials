{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Corpus with Textacy\n",
    "\n",
    "The Python `textacy` library builds on top of spaCy. Below we are going to create a corpus of texts by Tolkien fans using `textacy`. Then we'll use its built-in methods to do some analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import textacy\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy's English language model\n",
    "%run -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path to the file containing our data. In this case,\n",
    "# the data and metadata are stored together in one JSON file.\n",
    "records = textacy.io.read_json('data/records.json', lines=True)\n",
    "\n",
    "# We next want to create tuples containing our data and metadata.\n",
    "# We do this by splitting the file using textacy's split_records function.\n",
    "record_tuples = textacy.io.utils.split_records(records, 'text', itemwise=True)\n",
    "\n",
    "# We should be able to add all our records to a corpus, but on Binder\n",
    "# I needed an elaborate workaround. I put the records in a list and\n",
    "# created a corpus with the first record. Then I looped through the\n",
    "# rest of the list and added the rest of the records.\n",
    "record_list = []\n",
    "for r in record_tuples:\n",
    "    record_list.append(r)\n",
    "# Now we're ready to build the corpus. This can take a few minutes.\n",
    "corpus = textacy.Corpus(\n",
    "    textacy.load_spacy_lang(\"en_core_web_sm\", disable=(\"parser\", \"tagger\")),\n",
    "    data=record_list[0]\n",
    ")\n",
    "for record in record_list[1:]:\n",
    "    corpus.add_record(record)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get some quick statistics from the corpus\n",
    "print('Number of documents: ' + str(corpus.n_docs))\n",
    "print('Number of sentences: ' + str(corpus.n_sents))\n",
    "print('Number of tokens: ' + str(corpus.n_tokens))\n",
    "\n",
    "# Or preview a specific document\n",
    "corpus[9]._.preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a corpus, let's do some magic. `textacy` has some built in functions to count word vectors (lists of word counts for each text). With this information, it can make a document-term matrix showing the vectors for each document. We could view this matrix, but it is not very meaningful since it is just a bunch of numbers. So for now, let's just run it. `textacy` gives us a number of options which you can see in the `Vectorizer()` function. All you need to know is that we are running this with some settings that try to account for texts of different lengths, and we are ignoring extremely rare or frequent terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the textacy vectorizer and matrix module\n",
    "import textacy.vsm\n",
    "\n",
    "# Vectorize our corpus\n",
    "vectorizer = textacy.vsm.Vectorizer(\n",
    "     tf_type=\"linear\", apply_idf=True, idf_type=\"smooth\", norm=\"l2\",\n",
    "     min_df=2, max_df=0.95)\n",
    "\n",
    "# Create the document-term matrix\n",
    "doc_term_matrix = vectorizer.fit_transform(\n",
    "     (doc._.to_terms_list(ngrams=1, entities=True, as_strings=True)\n",
    "      for doc in corpus))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a document-term matrix, what can we do with it? Lots of things. But to keep things simple, let's employ a method of analysis called \"topic modelling\", which is built-in to `textacy`. Topic modelling makes three assumptions:\n",
    "\n",
    "1. All texts are made up of a finite set of themes/subjects/discources called \"topics\".\n",
    "2. These topics can be represented by lists of terms organised by the probability of each term occurring in the topic.\n",
    "3. An algorithm can be applied to a corpus of texts to reverse engineer the topics that make up those texts.\n",
    "\n",
    "There are many types of topic modelling algorithms. Two popuular ones are Latent Dirichlet Allocation (LDA) and Non-Negative Matrix Factorization (NMF).\n",
    "\n",
    "We feed our document-term matrix into a topic modelling engine with our desired settings, including the method and the number of topics we want to generate. We get a document-**topic** matrix back. Again, this is all numbers, but we can do some fancy magic in two lines to output a list of topics and the top (most \"probable\") terms in each topic. We'll do this below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the topic modelling module\n",
    "import textacy.tm  # note the import\n",
    "\n",
    "# Choose our modelling method and number of topics\n",
    "method = 'lda' # Latent Dirichlet Allocation\n",
    "# method = 'nmf' # Non-Negative Matrix Factorization\n",
    "model = textacy.tm.TopicModel(method, n_topics=10)\n",
    "\n",
    "# Produce the model and the document-topic matrix\n",
    "model.fit(doc_term_matrix)\n",
    "doc_topic_matrix = model.transform(doc_term_matrix)\n",
    "\n",
    "# Show a list of the top terms in each topic\n",
    "for topic_idx, top_terms in model.top_topic_terms(vectorizer.id_to_term, top_n=10):\n",
    "    print('topic', topic_idx, ':', '   '.join(top_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`textacy` provides a useful visualisation showing the weights of each term in each topic. If you used LDA, you probably got a bad model in which only one topic is very distinguishable. This is partially a result of our small data set. But try NMF and see if you get something better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.termite_plot(doc_term_matrix, vectorizer.id_to_term, topics=-1, n_terms=25, sort_terms_by=\"seriation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
