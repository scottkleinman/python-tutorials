{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with Python Libraries\n",
    "\n",
    "In this notebook, we will play with some Python libraries to perform some common tasks. W1 will:\n",
    "\n",
    "1. Use `requests` and `BeautifulSoup` to scrape a web page.\n",
    "2. Analyse the text using the `spaCy` natural language processing library.\n",
    "3. View the results in tables using `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import requests and BeautifulSoup\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# The web page to scrape\n",
    "url = 'https://www.fanfiction.net/s/6041872/1/Broken'\n",
    "\n",
    "# The requests library sends an HTTP request and returns a response\n",
    "response = requests.get(url)\n",
    "\n",
    "# BeautifulSoup converts the web page to a list of elements\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# We're going to guess where the content we want is in the page.\n",
    "# You may have to change this, depending on how the page is organised.\n",
    "content = soup.find('#storytext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the content, let's get all the paragraph tags\n",
    "# and join them into a single string.\n",
    "content = soup.find_all('p')\n",
    "paras = [p.text for p in content[2:]]\n",
    "text = ' '.join(paras)\n",
    "\n",
    "print(text[0:500] + '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download spaCy's English language model\n",
    "%run -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the spaCy Natural Language Processing (NLP) library\n",
    "import spacy\n",
    "\n",
    "# Load spaCy's language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process our text into a spaCy document\n",
    "doc = nlp(text)\n",
    "\n",
    "# Get some linguistic features\n",
    "for token in doc[0:5]:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks like it has some useful information, but it is hard to read!\n",
    "\n",
    "Time to play with pandas dataframes. A dataframe is a structure for holding data that can be easily viewed in a table. It's basically Excel for Python.\n",
    "\n",
    "We are going to assume that each token in a spaCy doc is a set of features (lemma, part of speech, etc.), and we want each token in its own row and each feature in its own column. To do this, we'll create a list of features with the features for each token in a dict. The dict keys will be the column names. Once we massage our data into that format, we can create the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load the document features into a dict\n",
    "features = []\n",
    "for token in doc:\n",
    "    feature = {\n",
    "        'token': token.text,\n",
    "        'norm': token.norm_,\n",
    "        'lemma': token.lemma_,\n",
    "        'pos': token.pos_,\n",
    "        'stopword': token.is_stop\n",
    "    }\n",
    "    features.append(feature)\n",
    "\n",
    "# Create a pandas dataframe\n",
    "df = pd.DataFrame(features, columns=['token', 'norm', 'lemma', 'pos', 'stopword'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can sort dataframes!\n",
    "\n",
    "sorted = df.sort_values('norm')\n",
    "# To reverse sort\n",
    "# sorted = df.sort_values('norm', ascending=False)\n",
    "\n",
    "# Show the sorted table\n",
    "sorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't want punctuation, spaces, digits, and stop words in our table. Take them out!\n",
    "tokens = [token.norm_ for token in doc if token.pos_ not in ['PUNCT', 'SPACE'] and token.norm_.isdigit() == False and token.is_stop == False]\n",
    "\n",
    "# Create a pandas dataframe, this time with just the lower-cased tokens\n",
    "df = pd.DataFrame(tokens, columns=['norm'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dict of the norms and counts\n",
    "counts = df['norm'].value_counts().to_dict()\n",
    "\n",
    "# Convert it to a list of dicts and feed to a new dataframe\n",
    "counts = [{'norm': k, 'count': v} for k, v in counts.items()]\n",
    "counted = pd.DataFrame(counts, columns=['norm', 'count'])\n",
    "\n",
    "# Show the counts\n",
    "counted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can even do some fancy plotting using Python's matplotlib library\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Some archane matplotlib stuff that experts understand and the rest of us Google\n",
    "ax = plt.gca()\n",
    "counted[0:10].plot(kind='line', x='norm', y='count', ax=ax, rot=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or we cna show it in a bar chart\n",
    "counted[0:10].plot(kind='bar', x='norm', y='count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `textacy`\n",
    "\n",
    "The Python `textacy` library builds on top of spaCy. Below we are going to create a corpus of texts by Tolkien fans using `textacy`. Then we'll use its built-in methods to do some analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import textacy\n",
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get keywords in context (KWIC)\n",
    "textacy.text_utils.KWIC(text, 'war', window_width=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use textacy instead of list comprehensions to scrub\n",
    "from textacy import preprocessing\n",
    "normalized_text = preprocessing.normalize_whitespace(preprocessing.remove_punctuation(text))\n",
    "normalized_text = textacy.preprocessing.replace.replace_numbers(normalized_text, '')\n",
    "normalized_text[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document statistics\n",
    "\n",
    "`textacy` can make spaCy docs. Once we have that done, we can use its `keywords` module to extract key phrases according to several different algoritms (the example shown below uses the \"TextRank\" algorithm).\n",
    "\n",
    "We can also get other kinds of statistics and even use `textacy` to produce term counts as we did above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the language model and make a spaCy doc\n",
    "en = textacy.load_spacy_lang('en_core_web_sm')\n",
    "doc = textacy.make_spacy_doc(text, lang=en)\n",
    "\n",
    "# Import the keywords module\n",
    "import textacy.ke\n",
    "print('Textrank:')\n",
    "print(textacy.ke.textrank(doc, normalize='lemma', topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get some text statistics\n",
    "\n",
    "stats = textacy.TextStats(doc)\n",
    "stats.basic_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the reading level for this text?\n",
    "\n",
    "stats.readability_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a bag of words using frequencies instead of counts\n",
    "bow = doc._.to_bag_of_terms(ngrams=(1), entities=False, weighting=\"freq\", as_strings=True)\n",
    "\n",
    "# Let's look at this in a dataframe\n",
    "bow = [{'Term': k, 'Frequency': v} for k, v in bow.items()]\n",
    "bow_df = pd.DataFrame(bow, columns=['Term', 'Frequency'])\n",
    "bow_df = bow_df.sort_values('Frequency', ascending=False)\n",
    "snippet = bow_df.head(15)\n",
    "\n",
    "# This just hides the dataframe index in a jupyter notebook, which is more pleasant on the eye\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(snippet.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've only played with one text. In the next notebook, we'll use `textacy` to build a corpus of texts and perform some low-level analysis. <a href=\"textacy.ipynb\" target=\"_blank\">Click here</a> to continue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
